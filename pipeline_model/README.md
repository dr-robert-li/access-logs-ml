# Access Log Classification Pipeline

This directory contains packaged pipeline models for analyzing and classifying web server access logs. These models combine log parsing, feature engineering, and machine learning classification into a single, self-contained package.

## Overview

The packaged pipeline models (`pipeline_*.pkl`) are self-contained Python objects that can:

1. Parse raw Apache/Nginx Apache HTTPD style access logs
2. Extract and engineer relevant features
3. Classify each log entry as:
   - Legitimate human traffic
   - Legitimate bot traffic
   - Illegitimate human traffic (manual attacks)
   - Illegitimate bot traffic (automated attacks)
4. Generate a comprehensive analysis summary

## Requirements

- Python 3.8+
- Required packages:
  - pandas
  - torch
  - transformers
  - tqdm
  - re
  - datetime

## Usage

Replace the model folder with an appropriately timestamped one of your own creation below.

### Basic Usage

```python
import pickle

# Load the pipeline
with open('pipeline_model/pipeline_20240520_123456.pkl', 'rb') as f:
    pipeline = pickle.load(f)

# Analyze logs from a file
results = pipeline.analyze_logs('path/to/your/access.log')

# Generate a summary
summary = pipeline.generate_summary(results)
print(summary)
```

### Analyzing Log Content (Instead of a File)

```python
# If you have log content as a string instead of a file
log_content = """
192.168.1.1 - - [20/May/2024:12:34:56 +0000] "GET /index.php HTTP/1.1" 200 1234 "https://example.com" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
192.168.1.2 - - [20/May/2024:12:35:00 +0000] "POST /wp-login.php HTTP/1.1" 403 789 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
"""

# Set is_file=False to indicate you're passing content directly
results = pipeline.analyze_logs(log_content, is_file=False)
```

### Saving Results

```python
# Save the classified results to a CSV file
results.to_csv('classified_logs.csv', index=False)

# Save the summary to a text file
with open('log_analysis_summary.md', 'w') as f:
    f.write(pipeline.generate_summary(results))
```

### Custom Prompt for Summary

```python
# You can provide a custom prompt to guide the summary generation
custom_prompt = "Focus on WordPress-related attacks and provide detailed recommendations for securing a WordPress site."
summary = pipeline.generate_summary(results, prompt=custom_prompt)
```

## Pipeline Methods

The pipeline object provides the following methods:

- `parse_logs(log_file_or_content, is_file=True)`: Parse raw logs into a structured DataFrame
- `enrich_logs(df)`: Add engineered features to the parsed logs
- `classify_logs(df_enriched)`: Classify each log entry using the trained model
- `analyze_logs(log_file_or_content, is_file=True)`: End-to-end analysis (combines the above steps)
- `generate_summary(df_classified, prompt=None)`: Generate a comprehensive analysis summary

## Creating a New Pipeline

To create a new packaged pipeline from a trained model:

```bash
python main.py package --model fine_tuned_model/model_20240520_123456
```

Or, directly and with more granularity in embedding a threat pattern file:

```bash
python src/package_pipeline.py --model fine_tuned_model/model_20240520_123456 --threat-patterns src/threat_int/malicious_req_patterns.txt
```

### Status Code Filtering

You can create pipelines that filter logs by HTTP status codes using regex patterns:

```bash
# Create a pipeline that only processes 4xx status codes
python main.py package --model fine_tuned_model/model_20240520_123456 --status-code-whitelist "4[0-9]{2}"

# Create a pipeline that excludes 2xx status codes
python main.py package --model fine_tuned_model/model_20240520_123456 --status-code-blacklist "2[0-9]{2}"

# Create a pipeline that only processes specific status codes (e.g., 403, 404, and 500)
python main.py package --model fine_tuned_model/model_20240520_123456 --status-code-whitelist "403|404|500"
```

This will create a new pipeline file in the `pipeline_model` directory with a timestamp in the filename.

## Example Output

The summary generated by the pipeline includes:

- Overview of total requests and time period
- Traffic classification breakdown (legitimate/illegitimate, human/bot)
- Examples of each traffic category
- Potential attacks detected and their types
- Malicious traffic sources
- Recommendations based on the analysis

## Troubleshooting

- **Memory Issues**: If you encounter memory errors with large log files, try processing the logs in chunks.
- **Missing Dependencies**: Ensure all required packages are installed.
- **Model Loading Errors**: Make sure you're using the correct version of PyTorch and Transformers.
- **Parsing Errors**: The parser expects standard Apache/Nginx log format. If your logs use a different format, you may need to modify the parsing logic.
- **Status Code Filtering**: If you're using a pipeline with status code filtering and getting unexpected results, check that your regex pattern is correct. Remember that whitelist means "only include these codes" and blacklist means "exclude these codes".

## Advanced Usage

### Processing Large Log Files

For very large log files, you can process them in chunks (again, replace the model folder with an appropriately timestamped one of your own creation):

```python
import pandas as pd

# Load the pipeline
with open('pipeline_model/pipeline_20240520_123456.pkl', 'rb') as f:
    pipeline = pickle.load(f)

# Process log file in chunks
chunk_size = 10000
all_results = []

for chunk in pd.read_csv('large_access_log.csv', chunksize=chunk_size):
    # Process each chunk
    chunk_results = pipeline.enrich_logs(chunk)
    chunk_results = pipeline.classify_logs(chunk_results)
    all_results.append(chunk_results)

# Combine all results
combined_results = pd.concat(all_results)

# Generate summary
summary = pipeline.generate_summary(combined_results)
```

### Using with Command Line Tools

You can use the pipeline with command-line tools like `grep` to pre-filter logs (again, replace the model folder with an appropriately timestamped one of your own creation):

```bash
# Filter logs for specific patterns and analyze them
grep "wp-login" access.log > filtered.log
python -c "import pickle; pipeline = pickle.load(open('pipeline_model/pipeline_20240520_123456.pkl', 'rb')); results = pipeline.analyze_logs('filtered.log'); print(pipeline.generate_summary(results))"
```

### Using Status Code Filtered Pipelines

If you've created a pipeline with status code filtering, you can check which filter is applied:

```python
import pickle

# Load the pipeline
with open('pipeline_model/pipeline_with_filter_20240520_123456.pkl', 'rb') as f:
    pipeline = pickle.load(f)

# Check if the pipeline has status code filters
if hasattr(pipeline, 'status_code_whitelist') and pipeline.status_code_whitelist:
    print(f"This pipeline only processes status codes matching: {pipeline.status_code_whitelist}")
elif hasattr(pipeline, 'status_code_blacklist') and pipeline.status_code_blacklist:
    print(f"This pipeline excludes status codes matching: {pipeline.status_code_blacklist}")
```

## License

*This software is provided as-is under the same license as the main access-log-classification project.*

MIT License

Copyright (c) 2025 Robert Li

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

## Disclaimer

This software is provided "as is" without warranty of any kind, either express or implied, including but not limited to the implied warranties of merchantability, fitness for a particular purpose, or non-infringement. The authors or copyright holders shall not be liable for any claim, damages, or other liability, whether in an action of contract, tort, or otherwise, arising from, out of, or in connection with the software or the use or other dealings in the software.

The classification results provided by this system should be used as guidance only and not as definitive security decisions. Always verify results and use in conjunction with other security measures.